{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Zhongyu/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract species for numeric encoding \n",
    "species = sorted(csv_data.species.unique())\n",
    "species_dict = {specie:index for index, specie in enumerate(species)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = csv_data.replace({'species':species_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Build a simple Neural Network using shape, margin, texture features to get a sense of performance\n",
    "**Split training set and testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(all_data.drop(['species'],axis=1), all_data['species'],\n",
    "                                                    test_size=330, stratify=all_data['species'], random_state=916)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>797</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>304</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>1031</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>1358</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1131</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "490   797  0.076172  0.123050  0.021484  0.003906  0.000000  0.158200   \n",
       "172   304  0.001953  0.000000  0.017578  0.011719  0.025391  0.000000   \n",
       "652  1031  0.033203  0.101560  0.011719  0.003906  0.001953  0.087891   \n",
       "848  1358  0.007812  0.001953  0.013672  0.015625  0.009766  0.005859   \n",
       "705  1131  0.060547  0.125000  0.007812  0.001953  0.000000  0.156250   \n",
       "\n",
       "      margin7  margin8   margin9    ...      texture55  texture56  texture57  \\\n",
       "490  0.000000      0.0  0.007812    ...       0.004883        0.0   0.039062   \n",
       "172  0.017578      0.0  0.003906    ...       0.031250        0.0   0.009766   \n",
       "652  0.027344      0.0  0.009766    ...       0.189450        0.0   0.034180   \n",
       "848  0.021484      0.0  0.000000    ...       0.042969        0.0   0.056641   \n",
       "705  0.001953      0.0  0.005859    ...       0.101560        0.0   0.027344   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "490   0.008789   0.029297        0.0        0.0   0.013672   0.000000   \n",
       "172   0.000000   0.020508        0.0        0.0   0.000000   0.032227   \n",
       "652   0.000000   0.018555        0.0        0.0   0.000000   0.000000   \n",
       "848   0.003906   0.015625        0.0        0.0   0.000000   0.000000   \n",
       "705   0.000000   0.000000        0.0        0.0   0.000000   0.000000   \n",
       "\n",
       "     texture64  \n",
       "490   0.074219  \n",
       "172   0.025391  \n",
       "652   0.003906  \n",
       "848   0.014648  \n",
       "705   0.003906  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data**\n",
    " - center data ??\n",
    " - one-hot-encode y\n",
    " - change data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = len(species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_encode = np.arange(num_labels)==np.array(train_y)[:,None]\n",
    "test_y_encode = np.arange(num_labels)==np.array(test_y)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y_encode = train_y_encode.astype(np.float32)\n",
    "test_y_encode = test_y_encode.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_numpy = train_X.drop(['id'],axis=1).as_matrix().astype(np.float32)\n",
    "test_X_numpy = test_X.drop(['id'],axis=1).as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size:\t(660, 192) \t(660, 99)\n",
      "testing set size:\t(330, 192) \t(330, 99)\n"
     ]
    }
   ],
   "source": [
    "print \"training set size:\\t\", train_X_numpy.shape, '\\t', train_y_encode.shape\n",
    "print \"testing set size:\\t\", test_X_numpy.shape, '\\t', test_y_encode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Simple Neural Network/Multi-Layer Preceptron Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_features = train_X_numpy.shape[1]\n",
    "hidden_layer_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(scope, shape, wt_initializer, center=True, scale=True):\n",
    "    with tf.variable_scope(scope, reuse=None) as sp:\n",
    "        wt = tf.get_variable(\"weights\", shape, initializer=wt_initializer)\n",
    "        bi = tf.get_variable(\"biases\", shape[-1], initializer=tf.constant_initializer(1.))\n",
    "        if center:\n",
    "            beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0))\n",
    "        if scale:\n",
    "            gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0))\n",
    "        moving_avg = tf.get_variable(\"moving_mean\", shape[-1], initializer=tf.constant_initializer(0.0), \\\n",
    "                                     trainable=False)\n",
    "        moving_var = tf.get_variable(\"moving_variance\", shape[-1], initializer=tf.constant_initializer(1.0), \\\n",
    "                                     trainable=False)\n",
    "        sp.reuse_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_lr = 0.001\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # prepare input \n",
    "    train_X_tf = tf.placeholder(tf.float32, shape=[batch_size, num_features])\n",
    "    train_y_tf = tf.placeholder(tf.float32, shape=[batch_size, num_labels])\n",
    "    valid_X_tf, valid_y_tf = tf.constant(test_X_numpy), tf.constant(test_y_encode)\n",
    "    \n",
    "    # initialize multi-layer parameters\n",
    "    layers = [{'scope':'hidden_layer', 'shape':[num_features, hidden_layer_size], \n",
    "               'initializer':tf.contrib.layers.variance_scaling_initializer()},\n",
    "              {'scope':'output_layer', 'shape':[hidden_layer_size, num_labels],\n",
    "               'initializer':tf.contrib.layers.variance_scaling_initializer()}]\n",
    "    for layer in layers:\n",
    "        initialize(layer['scope'], layer['shape'], layer['initializer'])\n",
    "    \n",
    "    # build model\n",
    "    def model(X, layer_scopes, is_training, keep_prob, decay=0.9):\n",
    "        for scope in layer_scopes:\n",
    "            with tf.variable_scope(scope, reuse=True):\n",
    "                wt = tf.get_variable(\"weights\")\n",
    "                bi = tf.get_variable(\"biases\")\n",
    "            X = tf.matmul(X, wt) + bi\n",
    "            # Batch Normalizaion\n",
    "            X = tf.contrib.layers.batch_norm(X, decay=decay, center=True, scale=True, is_training=is_training,\n",
    "                                            updates_collections=None, scope=scope, reuse=True)\n",
    "            # ReLu Activation\n",
    "            X = tf.nn.relu(X)\n",
    "            # Dropout for non-output layers\n",
    "            if scope!=layer_scopes[-1]:\n",
    "                X = tf.nn.dropout(X, keep_prob)\n",
    "        return X\n",
    "    \n",
    "    # setup a few parameters\n",
    "    layer_scopes = [l['scope'] for l in layers]\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # compute log loss logloss = - 1/N*Sum(yij*log(pij)) -> yij is 1 if observation i is in class j otherwise 0\n",
    "    train_logits = model(train_X_tf, layer_scopes, True, keep_prob)\n",
    "    train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits, train_y_tf))\n",
    "    valid_logits = model(valid_X_tf, layer_scopes, False, keep_prob)\n",
    "    valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(valid_logits, valid_y_tf))\n",
    "    \n",
    "    # setup optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = 0.001\n",
    "    #tf.train.exponential_decay(init_lr, global_step, decay_steps=5000, decay_rate=0.5, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch: 0:\tTrain Loss: 4.800885\tValid Loss: 5.460688\n",
      "Epoch: 100:\tTrain Loss: 0.602887\tValid Loss: 0.812202\n",
      "Epoch: 200:\tTrain Loss: 0.410338\tValid Loss: 0.600066\n",
      "Epoch: 300:\tTrain Loss: 0.351053\tValid Loss: 0.506743\n",
      "Epoch: 400:\tTrain Loss: 0.360860\tValid Loss: 0.465197\n",
      "Epoch: 500:\tTrain Loss: 0.206955\tValid Loss: 0.439483\n",
      "Epoch: 600:\tTrain Loss: 0.234630\tValid Loss: 0.412454\n",
      "Epoch: 700:\tTrain Loss: 0.227252\tValid Loss: 0.394866\n",
      "Epoch: 800:\tTrain Loss: 0.216372\tValid Loss: 0.378752\n",
      "Epoch: 900:\tTrain Loss: 0.184070\tValid Loss: 0.368429\n",
      "Epoch: 1000:\tTrain Loss: 0.256197\tValid Loss: 0.357826\n",
      "Epoch: 1100:\tTrain Loss: 0.237517\tValid Loss: 0.349896\n",
      "Epoch: 1200:\tTrain Loss: 0.051091\tValid Loss: 0.344473\n",
      "Epoch: 1300:\tTrain Loss: 0.059759\tValid Loss: 0.334552\n",
      "Epoch: 1400:\tTrain Loss: 0.351193\tValid Loss: 0.329385\n",
      "Epoch: 1500:\tTrain Loss: 0.179939\tValid Loss: 0.322167\n",
      "Epoch: 1600:\tTrain Loss: 0.106806\tValid Loss: 0.315126\n",
      "Epoch: 1700:\tTrain Loss: 0.044698\tValid Loss: 0.307684\n",
      "Epoch: 1800:\tTrain Loss: 0.322055\tValid Loss: 0.309056\n",
      "Epoch: 1900:\tTrain Loss: 0.115366\tValid Loss: 0.300414\n",
      "Epoch: 2000:\tTrain Loss: 0.097635\tValid Loss: 0.304712\n",
      "Epoch: 2100:\tTrain Loss: 0.096280\tValid Loss: 0.294273\n",
      "Epoch: 2200:\tTrain Loss: 0.234729\tValid Loss: 0.289235\n",
      "Epoch: 2300:\tTrain Loss: 0.126659\tValid Loss: 0.284487\n",
      "Epoch: 2400:\tTrain Loss: 0.083366\tValid Loss: 0.281879\n",
      "Epoch: 2500:\tTrain Loss: 0.164590\tValid Loss: 0.282903\n",
      "Epoch: 2600:\tTrain Loss: 0.162073\tValid Loss: 0.277153\n",
      "Epoch: 2700:\tTrain Loss: 0.099381\tValid Loss: 0.275090\n",
      "Epoch: 2800:\tTrain Loss: 0.091079\tValid Loss: 0.271466\n",
      "Epoch: 2900:\tTrain Loss: 0.159087\tValid Loss: 0.267490\n",
      "Epoch: 3000:\tTrain Loss: 0.161478\tValid Loss: 0.267588\n",
      "Epoch: 3100:\tTrain Loss: 0.018297\tValid Loss: 0.266547\n",
      "Epoch: 3200:\tTrain Loss: 0.017501\tValid Loss: 0.260456\n",
      "Epoch: 3300:\tTrain Loss: 0.158310\tValid Loss: 0.268873\n",
      "Epoch: 3400:\tTrain Loss: 0.155548\tValid Loss: 0.256938\n",
      "Epoch: 3500:\tTrain Loss: 0.079629\tValid Loss: 0.257637\n",
      "Epoch: 3600:\tTrain Loss: 0.010971\tValid Loss: 0.249179\n",
      "Epoch: 3700:\tTrain Loss: 0.296351\tValid Loss: 0.251244\n",
      "Epoch: 3800:\tTrain Loss: 0.083224\tValid Loss: 0.250185\n",
      "Epoch: 3900:\tTrain Loss: 0.077792\tValid Loss: 0.253439\n",
      "Epoch: 4000:\tTrain Loss: 0.078661\tValid Loss: 0.242583\n",
      "Epoch: 4100:\tTrain Loss: 0.366887\tValid Loss: 0.245247\n",
      "Epoch: 4200:\tTrain Loss: 0.009569\tValid Loss: 0.239670\n",
      "Epoch: 4300:\tTrain Loss: 0.078201\tValid Loss: 0.238207\n",
      "Epoch: 4400:\tTrain Loss: 0.148854\tValid Loss: 0.237976\n",
      "Epoch: 4500:\tTrain Loss: 0.147001\tValid Loss: 0.242111\n",
      "Epoch: 4600:\tTrain Loss: 0.081950\tValid Loss: 0.233281\n",
      "Epoch: 4700:\tTrain Loss: 0.074855\tValid Loss: 0.241871\n",
      "Epoch: 4800:\tTrain Loss: 0.077268\tValid Loss: 0.239948\n",
      "Epoch: 4900:\tTrain Loss: 0.151180\tValid Loss: 0.229560\n",
      "Epoch: 5000:\tTrain Loss: 0.080493\tValid Loss: 0.238533\n",
      "Epoch: 5100:\tTrain Loss: 0.077214\tValid Loss: 0.230451\n",
      "Epoch: 5200:\tTrain Loss: 0.149094\tValid Loss: 0.232016\n",
      "Epoch: 5300:\tTrain Loss: 0.148696\tValid Loss: 0.236038\n",
      "Epoch: 5400:\tTrain Loss: 0.003703\tValid Loss: 0.230241\n",
      "Epoch: 5500:\tTrain Loss: 0.003370\tValid Loss: 0.221895\n",
      "Epoch: 5600:\tTrain Loss: 0.220517\tValid Loss: 0.235709\n",
      "Epoch: 5700:\tTrain Loss: 0.145485\tValid Loss: 0.229052\n",
      "Epoch: 5800:\tTrain Loss: 0.073721\tValid Loss: 0.223079\n",
      "Epoch: 5900:\tTrain Loss: 0.004346\tValid Loss: 0.220621\n",
      "Epoch: 6000:\tTrain Loss: 0.289416\tValid Loss: 0.228886\n",
      "Epoch: 6100:\tTrain Loss: 0.075547\tValid Loss: 0.221755\n",
      "Epoch: 6200:\tTrain Loss: 0.073990\tValid Loss: 0.225187\n",
      "Epoch: 6300:\tTrain Loss: 0.073914\tValid Loss: 0.219879\n",
      "Epoch: 6400:\tTrain Loss: 0.217221\tValid Loss: 0.219177\n",
      "Epoch: 6500:\tTrain Loss: 0.076442\tValid Loss: 0.216040\n",
      "Epoch: 6600:\tTrain Loss: 0.073518\tValid Loss: 0.221041\n",
      "Epoch: 6700:\tTrain Loss: 0.145774\tValid Loss: 0.223024\n",
      "Epoch: 6800:\tTrain Loss: 0.144974\tValid Loss: 0.219082\n",
      "Epoch: 6900:\tTrain Loss: 0.073580\tValid Loss: 0.212814\n",
      "Epoch: 7000:\tTrain Loss: 0.073173\tValid Loss: 0.221673\n",
      "Epoch: 7100:\tTrain Loss: 0.145601\tValid Loss: 0.218410\n",
      "Epoch: 7200:\tTrain Loss: 0.146238\tValid Loss: 0.216766\n",
      "Epoch: 7300:\tTrain Loss: 0.074076\tValid Loss: 0.212038\n",
      "Epoch: 7400:\tTrain Loss: 0.073939\tValid Loss: 0.218266\n",
      "Epoch: 7500:\tTrain Loss: 0.074127\tValid Loss: 0.215976\n",
      "Epoch: 7600:\tTrain Loss: 0.151818\tValid Loss: 0.237915\n",
      "Epoch: 7700:\tTrain Loss: 0.001128\tValid Loss: 0.219270\n",
      "Epoch: 7800:\tTrain Loss: 0.001222\tValid Loss: 0.207264\n",
      "Epoch: 7900:\tTrain Loss: 0.360829\tValid Loss: 0.215496\n",
      "Epoch: 8000:\tTrain Loss: 0.144818\tValid Loss: 0.210023\n",
      "Epoch: 8100:\tTrain Loss: 0.072563\tValid Loss: 0.210383\n",
      "Epoch: 8200:\tTrain Loss: 0.072996\tValid Loss: 0.206341\n",
      "Epoch: 8300:\tTrain Loss: 0.359981\tValid Loss: 0.206047\n",
      "Epoch: 8400:\tTrain Loss: 0.073327\tValid Loss: 0.208690\n",
      "Epoch: 8500:\tTrain Loss: 0.072800\tValid Loss: 0.198647\n",
      "Epoch: 8600:\tTrain Loss: 0.072514\tValid Loss: 0.205276\n",
      "Epoch: 8700:\tTrain Loss: 0.144002\tValid Loss: 0.199782\n",
      "Epoch: 8800:\tTrain Loss: 0.073229\tValid Loss: 0.195741\n",
      "Epoch: 8900:\tTrain Loss: 0.072113\tValid Loss: 0.198791\n",
      "Epoch: 9000:\tTrain Loss: 0.144347\tValid Loss: 0.207479\n",
      "Epoch: 9100:\tTrain Loss: 0.144422\tValid Loss: 0.201657\n",
      "Epoch: 9200:\tTrain Loss: 0.073050\tValid Loss: 0.200673\n",
      "Epoch: 9300:\tTrain Loss: 0.072415\tValid Loss: 0.201162\n",
      "Epoch: 9400:\tTrain Loss: 0.144320\tValid Loss: 0.203641\n",
      "Epoch: 9500:\tTrain Loss: 0.144536\tValid Loss: 0.203401\n",
      "Epoch: 9600:\tTrain Loss: 0.000821\tValid Loss: 0.202789\n",
      "Epoch: 9700:\tTrain Loss: 0.000949\tValid Loss: 0.200784\n",
      "Epoch: 9800:\tTrain Loss: 0.216215\tValid Loss: 0.206850\n",
      "Epoch: 9900:\tTrain Loss: 0.144001\tValid Loss: 0.197253\n",
      "Finished training\n",
      "Final valid loss: 0.198222\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_y_encode.shape[0] - batch_size)\n",
    "        batch_X = train_X_numpy[offset: (offset+batch_size), :]\n",
    "        batch_y = train_y_encode[offset: (offset+batch_size), :]\n",
    "        feed_dict = {train_X_tf: batch_X, train_y_tf: batch_y, keep_prob: 0.8}\n",
    "        _, tloss = sess.run([optimizer, train_loss], feed_dict=feed_dict)\n",
    "        if step%100==0:\n",
    "            vloss = sess.run(valid_loss, feed_dict={keep_prob: 1.0})\n",
    "            print('Epoch: %d:\\tTrain Loss: %.6f\\tValid Loss: %.6f' \\\n",
    "                %(step, tloss, vloss))\n",
    "    print(\"Finished training\")\n",
    "    vloss = sess.run(valid_loss, feed_dict={keep_prob: 1.0})\n",
    "    print(\"Final valid loss: %.6f\" %(vloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
