{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Full Working Example of a 2-layer Neural Network with Batch Normalization (MNIST Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data, X shape\t(70000, 784)\n",
      "MNIST data, y shape\t(70000,)\n"
     ]
    }
   ],
   "source": [
    "print \"MNIST data, X shape\\t\", mnist.data.shape\n",
    "print \"MNIST data, y shape\\t\", mnist.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print mnist.data.dtype\n",
    "print mnist.target.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "mnist_X = mnist.data.astype(np.float32)\n",
    "mnist_y = mnist.target.astype(np.float32)\n",
    "print mnist_X.dtype\n",
    "print mnist_y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot-Encode y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 10)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "mnist_y = np.arange(num_classes)==mnist_y[:, None]\n",
    "mnist_y = mnist_y.astype(np.float32)\n",
    "print mnist_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split training, validation, testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\t\tFeatureShape\tLabelShape\n",
      "Training set:\t(50000, 784) \t(50000, 10)\n",
      "Validation set:\t(10000, 784) \t(10000, 10)\n",
      "Testing set:\t(10000, 784) \t(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(mnist_X, mnist_y, test_size=10000,\\\n",
    "                                                      random_state=102, stratify=mnist.target)\n",
    "train_X, test_X,  train_y, test_y  = train_test_split(train_X, train_y, test_size=10000,\\\n",
    "                                                     random_state=325, stratify=train_y)\n",
    "print 'Dataset\\t\\tFeatureShape\\tLabelShape'\n",
    "print 'Training set:\\t', train_X.shape,'\\t', train_y.shape\n",
    "print 'Validation set:\\t', valid_X.shape,'\\t', valid_y.shape\n",
    "print 'Testing set:\\t', test_X.shape, '\\t', test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a simple 2 layer neural network graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = train_X.shape[1]\n",
    "batch_size = 64\n",
    "hidden_layer_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize(scope, shape, wt_initializer, center=True, scale=True):\n",
    "    with tf.variable_scope(scope, reuse=None) as sp:\n",
    "        wt = tf.get_variable(\"weights\", shape, initializer=wt_initializer)\n",
    "        bi = tf.get_variable(\"biases\", shape[-1], initializer=tf.constant_initializer(1.))\n",
    "        if center:\n",
    "            beta = tf.get_variable(\"beta\", shape[-1], initializer=tf.constant_initializer(0.0))\n",
    "        if scale:\n",
    "            gamma = tf.get_variable(\"gamma\", shape[-1], initializer=tf.constant_initializer(1.0))\n",
    "        moving_avg = tf.get_variable(\"moving_mean\", shape[-1], initializer=tf.constant_initializer(0.0), \\\n",
    "                                     trainable=False)\n",
    "        moving_var = tf.get_variable(\"moving_variance\", shape[-1], initializer=tf.constant_initializer(1.0), \\\n",
    "                                     trainable=False)\n",
    "        sp.reuse_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_lr = 0.001\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # prepare input tensor\n",
    "    tf_train_X = tf.placeholder(tf.float32, shape=[batch_size, num_features])\n",
    "    tf_train_y = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "    tf_valid_X, tf_valid_y = tf.constant(valid_X), tf.constant(valid_y)\n",
    "    tf_test_X,  tf_test_y  = tf.constant(test_X),  tf.constant(test_y)\n",
    "    \n",
    "    # setup layers\n",
    "    layers = [{'scope':'hidden_layer', 'shape':[num_features, hidden_layer_size], \n",
    "               'initializer':tf.truncated_normal_initializer(stddev=0.01)},\n",
    "              {'scope':'output_layer', 'shape':[hidden_layer_size, num_classes],\n",
    "               'initializer':tf.truncated_normal_initializer(stddev=0.01)}]\n",
    "    # initialize layers\n",
    "    for layer in layers:\n",
    "        initialize(layer['scope'], layer['shape'], layer['initializer'])\n",
    "    \n",
    "    # build model - for each layer: -> X -> X*wt+bi -> batch_norm -> activation -> dropout (if not output layer) ->\n",
    "    layer_scopes = [layer['scope'] for layer in layers]\n",
    "    def model(X, layer_scopes, is_training, keep_prob, decay=0.9):\n",
    "        output_X = X\n",
    "        for scope in layer_scopes:\n",
    "            # X*wt+bi\n",
    "            with tf.variable_scope(scope, reuse=True):\n",
    "                wt = tf.get_variable(\"weights\")\n",
    "                bi = tf.get_variable(\"biases\")\n",
    "            output_X = tf.matmul(output_X, wt) + bi\n",
    "            # Insert Batch Normalization\n",
    "            output_X = tf.contrib.layers.batch_norm(output_X, decay=decay, center=True, scale=True,\n",
    "                                                    is_training=is_training,\n",
    "                                                    updates_collections=None, scope=scope, reuse=True)\n",
    "            # ReLu activation\n",
    "            output_X = tf.nn.relu(output_X)\n",
    "            # Dropout for all non-output layers\n",
    "            if scope!=layer_scopes[-1]:\n",
    "                output_X = tf.nn.dropout(output_X, keep_prob)\n",
    "        return output_X\n",
    "    \n",
    "    # setup keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # compute loss, make predictions\n",
    "    train_logits = model(tf_train_X, layer_scopes, True, keep_prob)\n",
    "    train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_logits, tf_train_y))\n",
    "    train_pred = tf.nn.softmax(train_logits)\n",
    "    valid_logits = model(tf_valid_X, layer_scopes, False, keep_prob)\n",
    "    valid_pred = tf.nn.softmax(valid_logits)\n",
    "    test_logits = model(tf_test_X, layer_scopes, False, keep_prob)\n",
    "    test_pred = tf.nn.softmax(test_logits)\n",
    "    \n",
    "    # compute accuracy\n",
    "    def compute_accuracy(predictions, labels):\n",
    "        correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    train_accuracy = compute_accuracy(train_pred, tf_train_y)\n",
    "    valid_accuracy = compute_accuracy(valid_pred, tf_valid_y)\n",
    "    test_accuracy  = compute_accuracy(test_pred , tf_test_y)\n",
    "    \n",
    "    # setup learning rate, optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(init_lr,global_step, decay_steps=500, decay_rate=0.95, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start a session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch: 0:\tLoss: 2.576616\t\tTrain Acc: 7.81%\tValid Acc: 29%\tLearning rate: 0.001000\n",
      "Epoch: 100:\tLoss: 0.745796\t\tTrain Acc: 89.06%\tValid Acc: 92%\tLearning rate: 0.001000\n",
      "Epoch: 200:\tLoss: 0.569677\t\tTrain Acc: 92.19%\tValid Acc: 94%\tLearning rate: 0.001000\n",
      "Epoch: 300:\tLoss: 0.608862\t\tTrain Acc: 85.94%\tValid Acc: 95%\tLearning rate: 0.001000\n",
      "Epoch: 400:\tLoss: 0.437363\t\tTrain Acc: 90.62%\tValid Acc: 95%\tLearning rate: 0.001000\n",
      "Epoch: 500:\tLoss: 0.369688\t\tTrain Acc: 95.31%\tValid Acc: 95%\tLearning rate: 0.000950\n",
      "Epoch: 600:\tLoss: 0.394675\t\tTrain Acc: 93.75%\tValid Acc: 96%\tLearning rate: 0.000950\n",
      "Epoch: 700:\tLoss: 0.442162\t\tTrain Acc: 89.06%\tValid Acc: 96%\tLearning rate: 0.000950\n",
      "Epoch: 800:\tLoss: 0.305682\t\tTrain Acc: 95.31%\tValid Acc: 96%\tLearning rate: 0.000950\n",
      "Epoch: 900:\tLoss: 0.361511\t\tTrain Acc: 90.62%\tValid Acc: 96%\tLearning rate: 0.000950\n",
      "Finished training\n",
      "Test accuracy: 97.210002%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "        batch_X = train_X[offset:(offset+batch_size), :]\n",
    "        batch_y = train_y[offset:(offset+batch_size), :]\n",
    "        feed_dict = {tf_train_X : batch_X, tf_train_y : batch_y, keep_prob : 0.6}\n",
    "        _, tloss, tacc = sess.run([optimizer, train_loss, train_accuracy], feed_dict=feed_dict)\n",
    "        vacc = sess.run(valid_accuracy, feed_dict={keep_prob : 1.0})\n",
    "        if step%100==0:\n",
    "            print('Epoch: %d:\\tLoss: %f\\t\\tTrain Acc: %.2f%%\\tValid Acc: %2.f%%\\tLearning rate: %.6f' \\\n",
    "                %(step, tloss, (tacc*100), (vacc*100), learning_rate.eval()))\n",
    "    print(\"Finished training\")\n",
    "    tacc = sess.run(test_accuracy, feed_dict={keep_prob : 1.0})\n",
    "    print(\"Test accuracy: %4f%%\" %(tacc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
